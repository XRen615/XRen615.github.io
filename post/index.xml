<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on INCH2</title>
    <link>http://xren615.github.io/post/</link>
    <description>Recent content in Posts on INCH2</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>xinyuyangren@foxmail.com (Rick)</managingEditor>
    <webMaster>xinyuyangren@foxmail.com (Rick)</webMaster>
    <lastBuildDate>Sun, 08 Jul 2018 20:58:01 +0800</lastBuildDate>
    
	<atom:link href="http://xren615.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Use Python on HDFS with Pig Latin UDF and Hadoop streaming</title>
      <link>http://xren615.github.io/post/hadoop_streaming/</link>
      <pubDate>Sun, 08 Jul 2018 20:58:01 +0800</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/hadoop_streaming/</guid>
      <description>在大多数情景中，当需要对HDFS上的数据做一些简单的ETL，我们常常直接选择Hive或者Apache Pig Latin来完成。而在其他少数情况下，如果想要插入其他脚本语言模块，如Python，来完成一些比较复杂的工作，这时我们一般有两种选择，UDF (User Defined Function) 或者Hadoop Streaming。
UDF with Python
It&amp;rsquo;s simple to use Py UDF with pig, just put a .py file and a .pig file under the same directory.
E.g.
udf_testing.py
@outputSchema(&#39;word:chararray&#39;) def hi_world(): return &amp;quot;hello world” def bingo(s): return s + &#39;bingo&#39;  udf_testing.pig
REGISTER &#39;udf_testing.py&#39; using jython as my_udfs; page_views = LOAD &#39;/data/tracking/PageViewEvent/&#39; USING LiAvroStorage(&#39;date.range&#39;, &#39;start.date=20171001;end.date=20171002;error.on.missing=false&#39;); hello_users = FOREACH page_views GENERATE requestHeader.pageKey, my_udfs.hi_world(), my_udfs.bingo(requestHeader.pageKey);; DUMP hello_users;  Note: limitation from Jython</description>
    </item>
    
    <item>
      <title>Toast :)</title>
      <link>http://xren615.github.io/post/toast/</link>
      <pubDate>Sun, 13 May 2018 17:39:00 +0800</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/toast/</guid>
      <description>I reserve my headline for @Leanne and propose a toast for her luck in next a few months
Praise the sun</description>
    </item>
    
    <item>
      <title>Random thoughts on A/B testing at LinkedIn</title>
      <link>http://xren615.github.io/post/experiment/</link>
      <pubDate>Fri, 13 Apr 2018 01:30:00 +0800</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/experiment/</guid>
      <description>写在前面
Random thoughts about A/B testing came into my mind while working at LinkedIn.
A/B测试正被越来越多的公司和科研机构使用，它能让决策告别『拍脑袋』，以实际的试验结果对比展示各个方案间的取舍，从而实现数据驱动的决策。LinkedIn作为业界最早以及最广泛使用A/B测试的科技公司之一，在方法论和工程实现上都有丰富的积累。本篇文章并非对其完整地论述，而是在日常工作中产生的随机总结和思考。
 Statistical ABC 本章节分级总结A/B测试的统计学基础。简单明了为先，严谨其次。
LEVEL1: 告别『拍脑袋』
这是树立数据驱动思想的第一步：开发者和产品经理都开始意识到，小至用绿色图标还是红色图标，大到内容流的构成和排序，都不能以『拍脑袋』的方式来决定：这种方法并不能帮助他们找到最优解，且倘若彼此意见不一致，彼此都很难说服对方。
于是他们产生了朴素的想法：给一半人展示绿色图标，同时给另一半人展示红色图标，然后比较两组试验的表现 —— 这就是A/B测试的基本思想。这样一来，A/B测试不仅可以回答『哪种方案更好？』而且可以通过比较定量地分析『好多少？』
LEVEL2: 我的差异显著吗
现在通过试验，产品经理喜爱的绿色和工程师喜爱的红色页面拥有了不同的表现数据。譬如点击率（CTP）对比是10% vs. 11% —— 那么，这足以说明后者更受用户欢迎吗？
除了关心试验数据本身，我们还需要考虑A/B测试结果中产生的组间差异有多大可能来自于随机误差。试想，同样是10% vs. 11%，10/100 vs. 11/100的说服力与1000/10000 vs. 1100/10000的说服力一样吗？直觉告诉我们，样本高达10000时显示出的1%差异似乎更有说服力，而100样本量实验中的仅仅1个转化的差异似乎有更大的可能来自于随机误差。
换而言之，在产品决策者做出决定之前，不仅需要知道实验组与对照组对比表现有多大提升，同时也需要知道观察到的提升有多大可能是来源于试验中的随机误差。
统计学中的假设检验理论为我们提供了量化这种可能性的方法。暂时略去其背后的理论体系，从实用的角度看，现在任何成熟的A/B测试系统都会随着试验结果一并给出一个叫做p-value的统计量，它将告诉我们两组实验数据的差异来自于随机误差的概率。一般地，行业标准可以接受的p-value范围是 &amp;lt;=0.05，换而言之，我们至少有95%的信心推断实验数据的差异确实来源于产品改动而非随机误差 —— 这时我们称组间差异是统计显著的。
LEVEL3 计算关键指标
对于不专门从事数据工作的职位，理解LEVEL2 已经达到了了解A/B测试的基础目标。然而数据分析师仍需要知其所以然，掌握各个关键指标背后的统计计算逻辑，才能有理有据地对实验数据进行详细分析。
仍然以点击率（CTP）为例，假设我们已知实验组的样本总量和点击量分别为（Nt, Xt），对照组的样本总量和点击量分别为（Nc, Xc），如何评价本次试验的显著性？
首先，可以很直观地计算出两个组别在实验中的点击率
$$Pt = \frac {Nt}{Xt} \quad and \quad Pc = \frac {Nc}{Xc}$$
我们进行原假设为『实验组和对照组的点击率没有明显差别』的假设检验，即 $$H_0: Pt - Pc = 0$$</description>
    </item>
    
    <item>
      <title>Python Data Engineering Cheat Sheet</title>
      <link>http://xren615.github.io/post/python_cheat_sheet/</link>
      <pubDate>Thu, 02 Mar 2017 17:40:40 +0800</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/python_cheat_sheet/</guid>
      <description>Some frequent needed utilities in Python data scripts —— good to have it by hand when facing puzzle.
ETL import matplotlib.pyplot as plt %matplotlib inline import numpy as np import pandas as pd import seaborn as sns  Data Loading
import pandas as pd  # From CSV df = pd.read_csv(&amp;quot;path&amp;quot;) # From Excel df = pd.read_excel(&#39;/path&#39;) # From database (sqlite) import sqlite3 conn = sqlite3.connect(&amp;quot;foo.db&amp;quot;) cur = conn.cursor() #Check how many tables are there in the database cur.</description>
    </item>
    
    <item>
      <title>A Simple Summary on Feature Selection</title>
      <link>http://xren615.github.io/post/feature_reducing/</link>
      <pubDate>Thu, 05 Jan 2017 19:19:22 +0100</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/feature_reducing/</guid>
      <description>介绍 数据工程项目往往严格遵循着riro (rubbish in, rubbish out) 的原则，所以我们经常说数据预处理是数据工程师或者数据科学家80%的工作，它保证了数据原材料的质量。而特征工程又至少占据了数据预处理的半壁江山，在实际的数据工程工作中，无论是出于解释数据或是防止过拟合的目的，特征选择都是很常见的工作。如何从成百上千个特征中发现其中哪些对结果最具影响，进而利用它们构建可靠的机器学习算法是特征选择工作的中心内容。在多次反复的工作后，结合书本，kaggle等线上资源以及与其他数据工程师的讨论，我决定写一篇简明的总结梳理特征选择工作的常见方法以及python实现。
总的来说，特征选择可以走两条路：
 特征过滤（Filter methods）: 不需要结合特定的算法，简单快速，常用于预处理
 包装筛选（Wrapper methods）: 将特征选择包装在某个算法内，常用于学习阶段
  在scikit-learn环境中，特征选择拥有独立的包sklearn.feature_selection, 包含了在预处理和学习阶段不同层级的特征选择算法。
A. 特征过滤（Filter methods） (1) 方差阈（Variance Treshhold）
最为简单的特征选择方式之一，去除掉所有方差小于设定值的特征。
在sklearn中实现：
from sklearn.feature_selection import VarianceThreshold   VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.</description>
    </item>
    
    <item>
      <title>SKII Recommender System Design</title>
      <link>http://xren615.github.io/post/sk2_rs/</link>
      <pubDate>Mon, 19 Dec 2016 13:21:09 +0800</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/sk2_rs/</guid>
      <description>Abstract This prototype finished during P&amp;amp;G Data Science Hackthon, Nov. 2016 in Cincinati, OH.
This document briefly elaborates the design ideology for SKII onsite recommender system, leverage the demographics data, skin scan result and purchase history.
Note. all the confidential data has been pre-processed.
The Dataset The data used in this recommender system consists of the folling 3 parts:
 Demographics information (1 million rows * 10 columns), including gender, age, marital status etc.</description>
    </item>
    
    <item>
      <title>Turn Your Social Account into a Weather Robot</title>
      <link>http://xren615.github.io/post/weibo_robot/</link>
      <pubDate>Sat, 02 Jul 2016 16:17:24 +0200</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/weibo_robot/</guid>
      <description>Demo This is my recent play-for-fun, a tiny trick developing a weatherman robot on SNS.
E.g. 
This robot will update the HK local weather everyday at 20:00 EDT and post it on my SinaWeibo account.
Checklist  A SinaWeibo developer account to use its post API (register here. You will need either a personal webpage or an application under development for registration). A WorldWeatherOnline API to update weather. They provide free API to fetch the weather worldwide and pack it in popular formats like xml, json or csv.</description>
    </item>
    
    <item>
      <title>Gaussian Mixture Models</title>
      <link>http://xren615.github.io/post/gmixture/</link>
      <pubDate>Sat, 16 Apr 2016 15:27:07 +0200</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/gmixture/</guid>
      <description>A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. Instead of having an output as cluster no. that a certain sample belongs to, Gaussian mixture model can additionally give an probability of this kind of clustering.
In practice, expectation-maximization (EM) algorithm is often used for fitting mixture-of-Gaussian models.
For better understand the theory of GMM this link could be followed.</description>
    </item>
    
    <item>
      <title>A data-based approach for behavior motivation digging</title>
      <link>http://xren615.github.io/post/edge_detection/</link>
      <pubDate>Tue, 15 Mar 2016 15:08:09 +0100</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/edge_detection/</guid>
      <description>Key Points This chapter briefly elaborates how to analyze the motivation of people&amp;rsquo;s operation on a system from the system electricity consumption signal and other data.
 Objective: understand how, and why occupants interact with the system. System: ventilation system in passive houses with adjustable flow rate option.
 Raw data: electricity consumption signal; environment sensor records (temperature, humidity, CO2 etc.); 3-min interval * 2 years (2013-2015): 325946 rows × 25 features.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>http://xren615.github.io/post/me/</link>
      <pubDate>Sat, 13 Feb 2016 17:34:00 +0800</pubDate>
      <author>xinyuyangren@foxmail.com (Rick)</author>
      <guid>http://xren615.github.io/post/me/</guid>
      <description>A person who sees himself as a(n) reader/engineer/gamer. Now works on data analytics.
contact: xinyuyangren@gmail.com</description>
    </item>
    
  </channel>
</rss>