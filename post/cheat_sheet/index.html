<!DOCTYPE html>
<html>

<head>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-76515451-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-76515451-1');
  </script>
  

  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Python Data Engineering Cheat Sheet  &middot; INCH2</title>
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <link rel="canonical" href="http://xren615.github.io/post/cheat_sheet/">
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css">
  <link rel="stylesheet" href="/css/styles.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<body>
  <div class="navbar">
    <ul>
      <li><a href="/" class="logo-title">HELLO,WORLD!</a></li>
      <li><a href="/categories">CATEGORIES</a></li>
      <li><a href="/tags">TAGS</a></li>
    </ul>
  </div>
  <div class="container">
    <div class="title">
      <a href="/">
        <img src="/images/logo.png" width="260px" height="48px" />
      </a>
    </div>


<a href="http://xren615.github.io/post/cheat_sheet/">
  <div class="post-title">
    <img src="/images/post-title-icon.png" />
    <div class="post-meta">
      <time>02 Mar 2017, 17:40</time>
      <h1>Python Data Engineering Cheat Sheet</h1>
    </div>
  </div>
</a>

<div class="post-toc">
  <span class="title">
    Table of contents
  </span>
  <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#etl">ETL</a></li>
<li><a href="#descriptive-stats">Descriptive Stats</a></li>
<li><a href="#feature-engineering">Feature Engineering</a></li>
<li><a href="#feature-selection">Feature Selection</a></li>
<li><a href="#algorithm">Algorithm</a></li>
<li><a href="#tuning-validation">Tuning &amp; Validation</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
</div>

<section class="post-content">
  

<p>Some frequent needed utilities in Python data scripts —— good to have it by hand when facing puzzle.</p>

<h3 id="etl">ETL</h3>

<hr />

<pre><code>import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
</code></pre>

<p><strong><em>Data Loading</em></strong></p>

<pre><code>import pandas as pd  

</code></pre>

<pre><code># From CSV
df = pd.read_csv(&quot;path&quot;)

# From Excel
df = pd.read_excel('/path')

# From database (sqlite)
import sqlite3
conn = sqlite3.connect(&quot;foo.db&quot;)
cur = conn.cursor()
#Check how many tables are there in the database
cur.execute(&quot;SELECT name FROM sqlite_master WHERE type='table';&quot;).fetchall()
#SQL Query to pandas dataframe
df = pd.read_sql_query(&quot;select * from bar limit 5;&quot;, conn)
</code></pre>

<p><strong><em>Indexing</em></strong></p>

<pre><code># Set index
df = df.set_index('colName')
# loc works on labels in the index
s.loc[:3]
# iloc works on the positions in the index (so it only takes integers)
s.iloc[:3]
</code></pre>

<p><strong><em>Sorting</em></strong></p>

<pre><code># First c1, then c2
df = df.sort(['c1','c2'], ascending=[False,True])
</code></pre>

<p><strong><em>Dropping</em></strong></p>

<pre><code># Drop columns (axis=0: column-wise; axis=1: row-wise)
df = df.drop(['Cabin','Ticket'],axis = 1)
# Drop rows
df = df.drop(['label string']) # by index name
df.drop(df.index[[1,3]]) # by row number (0-based)
</code></pre>

<p><strong><em>Slicing</em></strong></p>

<pre><code># Filter the dataset by a certain condition
df = df[df.name != 'Tina']
</code></pre>

<p><strong><em>Dealing with missings</em></strong></p>

<pre><code># Drop them
df = df.dropna()
# Fill them
df.fillna(0) # fill by a number
df.fillna(method='ffill') # propagates last valid observation forward to next valid
df.fillna(method='bfill')
</code></pre>

<p><strong><em>Sampling</em></strong></p>

<pre><code>df_GM_sample = df_GM.sample(n=None, frac=None)
</code></pre>

<p><strong><em>Apply function</em></strong></p>

<pre><code># axis=0: column-wise; axis=1: row-wise
df.apply(func,axis = )
# apply to every element
df.applymap(lambda x: )
</code></pre>

<p><strong><em>Dealing with datetime</em></strong></p>

<pre><code># string to datetime
df.dt = pd.to_datetime(df.dt, format='%Y%m%d')

# get datetime indexes
t = pd.DatetimeIndex(df.dt)
hr = t.hour
df['HourOfDay'] = hr
month = t.month
df['Month'] = month
year = t.year
df['Year'] = year

# resample time series
df = df.set_index('datetime')
weekly_summary['speed'] = df.speed.resample('W').mean()
weekly_summary['distance'] = df.distance.resample('W').sum()
weekly_summary['cumulative_distance'] = df.cumulative_distance.resample('W').last()

# generate given format string from datetime
df['DOB1'] = df['DOB'].dt.strftime('%m/%d/%Y')
</code></pre>

<p><strong><em>Categorical to dummy</em></strong></p>

<pre><code>dummiesT = pd.get_dummies(test['Embarked'],prefix = 'Embarked')
test = pd.concat([test,dummiesT],axis = 1)
test = test.drop('Embarked',axis =1)
</code></pre>

<p><strong><em>concat &amp; join</em></strong></p>

<pre><code># concat along rows
df_new = pd.concat([df_a, df_b])

# join
df = df1.join(df2, how='left', lsuffix='', rsuffix='', sort=False)
</code></pre>

<p><strong><em>Groupby</em></strong></p>

<pre><code>df.groupby(by = 'Sex').mean()
</code></pre>

<p><strong><em>Differencing &amp; Cumulation</em></strong></p>

<pre><code># Differencing
data['instantaneous'] = data.volume_out.diff()

# Cumulation
consum.loc[:,&quot;group&quot;] = consum[&quot;is_start_point&quot;].cumsum()
</code></pre>

<p><strong><em>Sliding Window Apply</em></strong></p>

<pre><code>df[&quot;is_lucky_than_previous&quot;] =\
pd.rolling_apply(df.Survived, 2, lambda x: x[1] - x[0] == 1).fillna(1)
</code></pre>

<p><strong><em>Regular Expression</em></strong></p>

<pre><code> def volCalc(row):
    name = row['tbordername']
    try:
        vol = 0
        p = re.compile(r'(\d+)ml')
        sizes = p.findall(name)
        for size in sizes:
            p1 = re.compile(size + r'ml\D+(\d)\D+')
            amount = p1.findall(name)
            if amount:
                vol += int(size)*int(amount[0])
            else:
                vol += int(size)*1
        return vol
    
    except:
        return 'N/A'
</code></pre>

<h3 id="descriptive-stats">Descriptive Stats</h3>

<hr />

<p><strong><em>Numerical stats</em></strong></p>

<pre><code>df.describe()
</code></pre>

<p><strong><em>Correlation</em></strong></p>

<pre><code>corr = df.corr()
plt.matshow(df.corr())
</code></pre>

<p><strong><em>Basic Charts</em></strong></p>

<pre><code># line chart
fig = plt.figure(figsize=(12,6))
plt.plot(data.dateTime,data.volume_out)
plt.title('title')

# hist: numerical feature distribution
df.Age.hist()
# categorical feature distribution  
df.Survived.value_counts().plot(kind = 'bar')
# Basic box plot
sns.boxplot(consum.instantaneous,orient='v')
plt.title('instantaneous consumption value distribution')
# Box plot with hue
sns.boxplot(x=&quot;Sex&quot;, y=&quot;Age&quot;,hue = 'Survived', data=df, palette=&quot;Set3&quot;)

# Scatter
plt.scatter(df.Fare,df.Survived)
plt.xlabel('Fare')
plt.ylabel('Survived?')

# Regression chart
sns.jointplot(x=&quot;duration&quot;, y=&quot;usage&quot;, kind = 'reg', data=filtered)
plt.title('title')


</code></pre>

<h3 id="feature-engineering">Feature Engineering</h3>

<hr />

<p><strong><em>Rescaling</em></strong></p>

<pre><code># (0,1) scaling 
# (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
cols_to_norm = ['PassengerId','SibSp']
df[cols_to_norm] = df[cols_to_norm].apply(lambda x: scaler.fit_transform(x))

# Standardization: Zero mean and unit variance
from sklearn.preprocessing import scale
cols_to_norm = ['Age','SibSp']
df[cols_to_norm] = df[cols_to_norm].apply(lambda x: scale(x))

# Normalization: scaling individual observation (row) to have unit norm.
# if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples, like KNN. 
from sklearn.preprocessing import normalize
df_normalized = pd.DataFrame(normalize(df._get_numeric_data(),norm = 'l2'),columns=df._get_numeric_data().columns,index=df._get_numeric_data().index)
df_normalized.apply(lambda x: np.sqrt(x.dot(x)), axis=1) # check results
</code></pre>

<p><strong><em>Feature Binarization</em></strong></p>

<pre><code># thresholding numerical features to get boolean values
from sklearn.preprocessing import Binarizer
binarizer = Binarizer(threshold=30)
df['Age'] = df['Age'].apply(lambda x: binarizer.fit_transform(x)[0][0])
</code></pre>

<p><strong><em>Generating Polynomial Features</em></strong></p>

<pre><code># get features’ high-order and interaction terms
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(2)
#  (X_1, X_2) to (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)
X_poly = pd.DataFrame(poly.fit_transform(X))
</code></pre>

<h3 id="feature-selection">Feature Selection</h3>

<hr />

<p><strong><em>Filter methods</em></strong></p>

<pre><code># Variance Treshhold
from sklearn.feature_selection import VarianceThreshold 

# Univariate feature selection 
X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
</code></pre>

<p><strong><em>Wrapper Methods</em></strong></p>

<pre><code># LASSO
class sklearn.linear_model.Lasso()

# Tree-based
class sklearn.ensemble.RandomForestClassifier()
</code></pre>

<h3 id="algorithm">Algorithm</h3>

<hr />

<p><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/"><strong><em>Sk-Learn Official Cheat Sheet</em></strong></a>
<div  align="center"><br />
<img src="http://7xro3y.com1.z0.glb.clouddn.com/sklearncs.png" align=center width = "800" height = "500"/><br />
</div></p>

<p><strong><em>Frequent Used Pieces</em></strong></p>

<p><strong><em>Linear Regression</em></strong></p>

<pre><code>from sklearn import linear_model
# Create linear regression object
regr = linear_model.LinearRegression(fit_intercept=True)
# Train the model using the training sets
regr.fit(df, y)
# The coefficients
print('Coefficients:', regr.coef_)
# The mean squared error
print(&quot;Mean squared error: %.2f&quot;
      % np.mean((regr.predict(df) - y) ** 2))
# Explained variance score: 1 is perfect prediction
print &quot;R Squared score:&quot;;regr.score(df, y)

# Coef_ check
plt.figure(figsize=(12,8))
plt.barh(range(len(regr.coef_)),regr.coef_,height=0.2,tick_label = df.columns)
plt.title('Regression Coefficients')

# Residuals Check
res = regr.predict(df) - y
plt.axhline(0)
plt.scatter(range(len(res)),res.values,color = 'r')
plt.title('Residual Plot')
</code></pre>

<p><strong><em>Kmeans</em></strong></p>

<pre><code>from sklearn.cluster import KMeans
estimator = KMeans(n_clusters=3)
estimator.fit(filtered_scaled)
labels = estimator.labels_
</code></pre>

<p><strong><em>Random Forest</em></strong></p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
rf = RandomForestClassifier(n_estimators=8000,n_jobs=-1,oob_score=True)
rf.fit(train,res)
rf.oob_score_
# feature importance
feature_importances = pd.Series(rf.feature_importances_,index = train.columns)
feature_importances.sort(inplace = True)
feature_importances.plot(kind = 'barh')
</code></pre>

<p><strong><em><a href="https://turi.com">Recommender</a></em></strong></p>

<pre><code># item-based CF
import graphlab
train_data = graphlab.SFrame(df_CF)
item_sim_model = graphlab.item_similarity_recommender.create(train_data, user_id='Customer_id', item_id='item')
# Make Recommendations
item_sim_recomm = item_sim_model.recommend(users=['5208494361'],k=10)
item_sim_recomm.print_rows()
</code></pre>

<p><strong><em>Time Series</em></strong></p>

<pre><code># DF Test
from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries):
    
    # Determing rolling statistics
    rolmean = pd.rolling_mean(timeseries, window=10)
    rolstd = pd.rolling_std(timeseries, window=10)

    # Plot rolling statistics:
    plt.figure(figsize=(14,8))
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    
    plt.legend(loc='best')
    plt.title('Rolling Mean &amp; Standard Deviation')
    plt.show(block=False)
    
    # Perform Dickey-Fuller test:
    print 'Results of Dickey-Fuller Test:'
    dftest = adfuller(timeseries.SALES_IN_ML, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print dfoutput
</code></pre>

<pre><code># ARIMA
# Ordering: ACF and PACF plots
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(TS_log, lags=30)
plot_pacf(TS_log, lags=30)

# Ordering: AIC
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.stattools import arma_order_select_ic
# Smaller Better
arma_order_select_ic(TS_log, max_ar=4, max_ma=0, ic='aic')

# Modeling
model = ARIMA(TS_log, order=(1, 0, 0))  
results_AR = model.fit(disp= 1)  
plt.plot(TS_log)
plt.plot(results_AR.fittedvalues, color='red')
TS_fitted = pd.DataFrame(results_AR.fittedvalues,columns=['SALES_IN_ML'])

# Residual Series Check
residuals = TS_log - TS_fitted
test_stationarity(residuals)
</code></pre>

<h3 id="tuning-validation">Tuning &amp; Validation</h3>

<hr />

<p><strong><em>Training/Test split</em></strong></p>

<pre><code>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)
</code></pre>

<p><strong><em>Cross Validation</em></strong></p>

<pre><code>from sklearn.model_selection import cross_val_score
from sklearn import svm
clf = svm.SVC(kernel='linear', C=1)
scores = cross_val_score(clf, X.values, y[0].values, cv=5)
# 95% confidence interval of the score
print(&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() * 2))
</code></pre>

<p><strong><em>Exhaustive Grid Search</em></strong></p>

<pre><code>from sklearn.model_selection import GridSearchCV
svr = svm.SVC()
parameters = {'kernel':('linear','rbf'), 'C':[1, 10]}
clf = GridSearchCV(svr, parameters,n_jobs = -1,cv = 5)
clf.fit(X, y[0])
clf.cv_results_
clf.best_estimator_
</code></pre>

</section>

<div class="share">
  <a href="http://www.facebook.com/sharer.php?src=bm&u=http%3a%2f%2fxren615.github.io%2fpost%2fcheat_sheet%2f&t=Python%20Data%20Engineering%20Cheat%20Sheet" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-facebook"></i></a>
  <a href="http://twitter.com/intent/tweet?url=http%3a%2f%2fxren615.github.io%2fpost%2fcheat_sheet%2f&text=Python%20Data%20Engineering%20Cheat%20Sheet&tw_p=tweetbutton" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-twitter"></i></a>
  <a href="https://plus.google.com/share?url=http%3a%2f%2fxren615.github.io%2fpost%2fcheat_sheet%2f" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-google-plus"></i></a>
  <a href="http://getpocket.com/edit?url=http%3a%2f%2fxren615.github.io%2fpost%2fcheat_sheet%2f&title=Python%20Data%20Engineering%20Cheat%20Sheet" onclick="window.open(this.href, 'PCwindow', 'width=550, height=350, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fa fa-get-pocket"></i></a>
</div>

<div class="post-meta-code">
  <div class="desc">
    public string description() {
    <div class="desc">val author = "
    
        X.Ren
    ";
    </div>
    
    }
  </div>
  <div class="desc">
    public string tags() {
      <div class="desc">
        val tags = [
        
          
          <a href="http://xren615.github.iotags/python">"python"</a>
          
          <a href="http://xren615.github.iotags/cheat-sheet">"cheat sheet"</a>
          
        
        ];
      </div>
    }
  </div>
</div>


<div class="post-comment">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "riren" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<div class="paging post-paging">
  
  <a class="left" href="http://xren615.github.io/post/experiment/" rel="prev">
    <i class="fa fa-caret-left" aria-hidden="true"></i> <span>Random thoughts on A/B testing at LinkedIn</span>
  </a>
  
  
  <a class="right" href="http://xren615.github.io/post/feature_reducing/" rel="next">
    <span>A Simple Summary on Feature Selection</span> <i class="fa fa-caret-right" aria-hidden="true"></i>
  </a>
  
</div>

<footer class="footer">
  COPYRIGHT (C) <a href="https://blog.lulab.net">DONGGEUN,BANG</a>. ALL RIGHTS RESERVED.
</footer>

</body>
</html>

